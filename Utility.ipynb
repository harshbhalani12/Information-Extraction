{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printPOS(statement):\n",
    "    doc = nlp(statement)\n",
    "    print(\"\\n\\n-------------General info------\")\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEntities(statement):\n",
    "    doc = nlp(statement)\n",
    "    print(\"\\n\\n-------------Entities------\")\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isExpectedVerbToken(token):\n",
    "    return token.lemma_ in [\"release\",\"launch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterVerbTokens(statement):\n",
    "    doc = nlp(statement)\n",
    "    result = list(filter(lambda token: token.pos_ == \"VERB\" and token.lemma_ != \"be\", doc))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_tree_nodes(sentence):\n",
    "    nodes = []\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        nodes.append([token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children]])\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDependencies(sentence):\n",
    "    nodes = get_dependency_tree_nodes(sentence)\n",
    "    print(\"\\n\\n------------- Deps ------\")\n",
    "    for node in nodes:\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNounChunks(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    print(\"\\n\\n------------- Noun chunks ------\")\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(chunk.text+ \" start:\"+str(chunk.start_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEverything(sentence):\n",
    "    printPOS(sentence)\n",
    "    printEntities(sentence)\n",
    "    printNounChunks(sentence)\n",
    "    printDependencies(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-------------General info------\n",
      "On on ADP IN prep Xx True False\n",
      "April april PROPN NNP pobj Xxxxx True False\n",
      "10 10 NUM CD nummod dd False False\n",
      ", , PUNCT , punct , False False\n",
      "2017 2017 NUM CD nummod dddd False False\n",
      ", , PUNCT , punct , False False\n",
      "filming filming NOUN NN nsubjpass xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "Fallout fallout PROPN NNP pobj Xxxxx True False\n",
      "was be VERB VBD auxpass xxx True True\n",
      "slated slat VERB VBN ROOT xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "start start VERB VB xcomp xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "Paris paris PROPN NNP pobj Xxxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "\n",
      "\n",
      "-------------Entities------\n",
      "April 10, 2017 3 17 DATE\n",
      "Paris 61 66 GPE\n",
      "\n",
      "\n",
      "------------- Noun chunks ------\n",
      "April start:3\n",
      "filming start:19\n",
      "Fallout start:30\n",
      "Paris start:61\n",
      "\n",
      "\n",
      "------------- Deps ------\n",
      "['On', 'prep', 'slated', 'VERB', [April]]\n",
      "['April', 'pobj', 'On', 'ADP', [10, ,, 2017]]\n",
      "['10', 'nummod', 'April', 'PROPN', []]\n",
      "[',', 'punct', 'April', 'PROPN', []]\n",
      "['2017', 'nummod', 'April', 'PROPN', []]\n",
      "[',', 'punct', 'slated', 'VERB', []]\n",
      "['filming', 'nsubjpass', 'slated', 'VERB', [of]]\n",
      "['of', 'prep', 'filming', 'NOUN', [Fallout]]\n",
      "['Fallout', 'pobj', 'of', 'ADP', []]\n",
      "['was', 'auxpass', 'slated', 'VERB', []]\n",
      "['slated', 'ROOT', 'slated', 'VERB', [On, ,, filming, was, start, .]]\n",
      "['to', 'aux', 'start', 'VERB', []]\n",
      "['start', 'xcomp', 'slated', 'VERB', [to, in]]\n",
      "['in', 'prep', 'start', 'VERB', [Paris]]\n",
      "['Paris', 'pobj', 'in', 'ADP', []]\n",
      "['.', 'punct', 'slated', 'VERB', []]\n"
     ]
    }
   ],
   "source": [
    "s1 = \"Filming of Ironman began in April at Edwards Air Force Base and ended on May 2.\"\n",
    "s3 =\"On April 10, 2017, filming of Fallout was slated to start in Paris.\"\n",
    "sentence = s3\n",
    "printEverything(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depNodes(statement):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
